{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyPWMtPcTzBA8Ym8rlnIfKkB"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","source":[" **Can Gradient Descent get stuck in a local minimum when training a Logistic\n","Regression model?**"],"metadata":{"id":"Y5d1-u_wBWsr"}},{"cell_type":"markdown","source":["For logistic regression, the cost function is log-loss or cross-entropy loss is convex.\n","\n","Convex Function: A convex function has a shape where, if you draw a line segment between any two points on the graph, the segment lies above or on the graph. This property ensures that there is a single global minimum and no local minima or maxima\n","\n","This means that for logistic regression, the cost function's convexity ensures that gradient descent will not get stuck in local minima because there are no local minimaâ€”only a single global minimum.\n","\n","Global Minimum: Since the cost function for logistic regression is convex, gradient descent will theoretically converge to the global minimum given enough iterations and an appropriate learning rate."],"metadata":{"id":"4GUVMdQiBYdD"}},{"cell_type":"markdown","source":["**Is it a good idea to stop Mini-batch Gradient Descent immediately when the validation error goes up?**"],"metadata":{"id":"WNkdZWqzCUCb"}},{"cell_type":"markdown","source":["Stopping Mini-batch Gradient Descent immediately when the validation error goes up can be a good idea, and it helps prevent overfitting and saves computational resources.\n","\n","When we say \"validation error goes up,\" it means that the error on the validation dataset is increasing over time as the model is being trained. This increase is a signal that the model's ability to generalize to new data may be deteriorating.\n","\n","Importance of Monitoring Validation Error\n","Overfitting: If the validation error starts to increase after a period of decreasing or stabilizing, it indicates that the model is starting to overfit the training data. Overfitting occurs when the model becomes too complex and starts to memorize noise and details specific to the training data that do not generalize well to new data.\n","\n","Early Stopping: Monitoring the validation error allows us to implement techniques like early stopping. Early stopping involves stopping the training process as soon as the validation error stops improving or starts to deteriorate. This helps prevent the model from overfitting and can improve its generalization performance.\n"],"metadata":{"id":"44e2SSarCnvK"}},{"cell_type":"code","source":[],"metadata":{"id":"qJcM46rHBWYK"},"execution_count":null,"outputs":[]},{"cell_type":"code","execution_count":null,"metadata":{"id":"7CY9yQvOBTgf"},"outputs":[],"source":[]}]}